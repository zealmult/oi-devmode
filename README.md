# oi-devmode

A lightweight developer logging filter designed for Open WebUI / OpenAI-compatible middleware.
It provides clean, structured, auto-formatted logs for:
	â€¢	Incoming requests (INLET)
	â€¢	Streaming responses (STREAM)
	â€¢	Final assistant messages (OUTLET)

The V3 version introduces full compatibility with mixed event types â€” handling both
dict (frontend events) and bytes (API streaming events) without breaking the stream pipeline.

â¸»

âœ¨ Features

ğŸ” Developer Logging (Toggle Anytime)

Enable or disable detailed logging using a simple boolean switch.
When disabled, the middleware stays completely silent and cost-free.

ğŸ“¥ INLET Logging (User Requests)

Captures the latest incoming user message, including optional display of the entire __user__ dictionary.

ğŸ“¤ OUTLET Logging (Saved Responses)

Captures the assistantâ€™s final response before saving to frontend.

ğŸ“¡ STREAM Logging (Real-time AI Output)

Supports both:
	â€¢	Frontend delta (dict format)
	â€¢	OpenAI API streaming chunks (bytes format)

Automatically detects type and logs appropriately, ensuring compatibility with custom models, proxies, and unusual streaming formats.

ğŸ”§ Configurable Valves

Control logging behavior with fine-grained options:
	â€¢	enabled
	â€¢	log_inlet
	â€¢	log_stream
	â€¢	log_outlet
	â€¢	log_user_info
	â€¢	truncate_message
	â€¢	priority

â¸»

ğŸ“¦ Installation

Add the Filter class into your Open WebUI plugin or Python middleware.

your_project/
â””â”€â”€ filters/
    â””â”€â”€ oi_devmode.py

Import it in your filter registry:

from filters.oi_devmode import Filter

Open WebUI will automatically load it.

â¸»

ğŸ›  Usage

Enable logging temporarily

filter = Filter()
filter.valves.enabled = True

Disable after debugging

filter.valves.enabled = False

Truncate long messages (optional)

filter.valves.truncate_message = 200


â¸»

ğŸ§ª Example Log Output

------------------------------ [DEV_LOGGER | INLET] ------------------------------
USER: test@example.com (Role: admin)
MODEL: gpt-4.1
MESSAGE (Role: user):
Hello, can you help me debug this?

__USER__ dictionary details:
{
  "email": "test@example.com",
  "role": "admin"
}
------------------------------------------------------------------------------

STREAM logs:

[DEV_LOGGER | STREAM] (Dict) AI streaming: Sure, let me check...

Or bytes-format API:

[DEV_LOGGER | STREAM] (Bytes) AI streaming: {"delta":{"content":"Hello"}}


â¸»

ğŸ“˜ API Summary

inlet()

Logs incoming user messages.

stream()

Logs every streamed partial output from the model.

outlet()

Logs the final assistant message and metadata.

All hooks return the original event unchanged, preserving pipeline integrity.

â¸»

ğŸ§­ When to Use This

âœ” Debugging custom models
âœ” Developing new filters or pipelines
âœ” Tracing AI answers in detail
âœ” Monitoring user requests during testing
âœ” Capturing malformed API streaming events

â¸»

ğŸš« When NOT to Use

âœ– Production (unless you purposely want verbose logs)
âœ– Handling sensitive user info (remember log_user_info=True)
âœ– High-throughput workloads (log spam may degrade performance)

â¸»

ğŸ¤ Contributing

Pull requests and improvements are welcome!
Submit issues and ideas via GitHub.

â¸»

â¤ï¸ Support the Project

If this tool improves your workflow, consider supporting:

https://breathai.top/

	â€¢	æˆ–è€…æƒ³æˆ‘å¸®ä½ åˆ›å»º PyPI å‘å¸ƒç‰ˆæœ¬

æˆ‘ä¹Ÿèƒ½ç»§ç»­å¸®ä½ ã€‚
